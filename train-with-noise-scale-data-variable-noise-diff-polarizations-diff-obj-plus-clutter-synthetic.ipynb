{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"../UXO_protected/+BTInvertPY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cmap\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import uxo_utils\n",
    "from uxo_utils import (\n",
    "    SensorInfo, Model, preCalcLoopCorners, FModParam, \n",
    "    forwardWithQ, sensorCoords2RxCoords, hprimary, formQmatrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 14\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load sensor info and ordnance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorinfo = uxo_utils.load_sensor_info()\n",
    "ordnance = uxo_utils.load_ordnance_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['105mm', '2.36-in', '2.75-in', '2.95-in', '20-lb Bomb', '25-lb Bomb', '3-in', '3.5-in', '30mm', '37mm', '4-in', '4-in Sphere', '4.2-in', '4.5-in', '4.52-in', '5-lb Bomb', '57mm', '60mm', '66mm', '75mm', '76mm', '81mm', '90mm', '106mm', 'ISO Large', 'ISO Medium', 'ISO Small', '2.25-in', '10-lb Bomb', '100-lb Bomb', 'Booster', 'Fuze', '120mm', 'Grenade', '152mm', 'Igniter Bomb', 'Landmine', 'Livens Projectile', 'Depth Charge', 'Parachute Flare', 'Rifle Grenade', '155mm', 'Dual Mode HE Rocket', 'Smoke Pot', 'Underwater Mine', '16-in', '175mm', 'Rocket Motor', '20mm', '250-lb Bomb', '25mm', '35mm', '40mm', '105mm SABOT', '5-in', '50 cal', '6-in', '7.2-in Depth Charge', '8-in', 'Bomblet', 'Booster Cup'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordnance.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up survey parameters\n",
    "- x is cross-line\n",
    "- y is inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntx = len(sensorinfo.transmitters)\n",
    "ymax = 3. \n",
    "y_spacing = 0.2\n",
    "dy = y_spacing / ntx\n",
    "nloc = int(ymax/dy)\n",
    "ncycles = int(nloc/ntx)\n",
    "\n",
    "y = np.linspace(0, ymax-dy, nloc)\n",
    "x = np.zeros(nloc)\n",
    "z = 0.28 * np.ones(nloc)\n",
    "\n",
    "pitch = np.zeros(nloc)\n",
    "roll = np.zeros(nloc)\n",
    "yaw = np.zeros(nloc)  # moving north (sensor in typical orientation)\n",
    "\n",
    "txnum = np.kron(np.ones(ncycles), np.arange(ntx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sensor location coordinates to Rx locations\n",
    "pos, mnum = sensorCoords2RxCoords(\n",
    "    sensorinfo=sensorinfo,\n",
    "    x = x, \n",
    "    y = y, \n",
    "    z = z, \n",
    "    pitch = pitch, \n",
    "    roll = roll, \n",
    "    yaw = yaw,\n",
    "    txnum = txnum\n",
    ")\n",
    "\n",
    "pitch = np.concatenate([np.tile(x,pos[i].shape[0]) for i,x in enumerate(pitch)])\n",
    "roll = np.concatenate([np.tile(x,pos[i].shape[0]) for i,x in enumerate(roll)])\n",
    "yaw = np.concatenate([np.tile(x,pos[i].shape[0]) for i,x in enumerate(yaw)])\n",
    "pos = np.concatenate(pos,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0\n",
      "-0.25 3.21\n",
      "0.28 0.28\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(pos[:, i].min(), pos[:, i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ordnance objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ordnance[\"ISO Small\"][\"times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_me = {\n",
    "    \"ISO Small\": 1,\n",
    "    \"ISO Medium\": 1,\n",
    "    \"ISO Large\": 1,\n",
    "    \"20mm\": 1\n",
    "}\n",
    "fig, ax = plt.subplots(len(plot_me.keys()), 2, figsize=(10, 4*len(plot_me.keys())))\n",
    "\n",
    "for i, key in enumerate(plot_me.keys()):\n",
    "    for l in [\"L1\", \"L2\", \"L3\"]:\n",
    "        L = ordnance[key][l][plot_me[key]]\n",
    "        ax[i, 0].plot(times, L, label=l)\n",
    "        ax[i, 1].loglog(times, L, label=l)\n",
    "\n",
    "    for a in ax[i, :]:\n",
    "        a.set_title(f\"{key}, index: {1}\")\n",
    "        a.grid(which=\"both\")\n",
    "        a.legend()\n",
    "        a.set_xlabel(\"time (ms)\")\n",
    "        a.set_ylim([1e-3, 20])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ranges of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_ranges = [\n",
    "    np.r_[0, 0.5], \n",
    "    np.r_[0, 0.5],\n",
    "    np.r_[0, 0.5],\n",
    "    np.r_[0, 0.5],\n",
    "]\n",
    "\n",
    "x_range = np.r_[-1.25, 1.25]\n",
    "y_range = np.r_[0., ymax]\n",
    "\n",
    "yaw_range = np.r_[0, 2*np.pi]\n",
    "pitch_range = np.r_[0, 2*np.pi]\n",
    "roll_range = np.r_[0, 2*np.pi]\n",
    "\n",
    "noise_amplitudes = np.r_[0, 0.1]\n",
    "\n",
    "def generate_random_variables(n, bounds):\n",
    "    return bounds.min() + (bounds.max() - bounds.min()) * np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_test_plot = np.load(\"noise_test_plot.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the data by mn ind\n",
    "noise_test_plot[\"data\"].shape\n",
    "noise_test_plot_mn = [noise_test_plot[\"data\"][:, noise_test_plot[\"mn\"] == i+1] for i in range(mnum.max()+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noise_test_plot_mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 2048*4\n",
    "ntest = 1024*2\n",
    "nvalid = 1024*2\n",
    "\n",
    "class_dict = {\n",
    "    0: \"not TOI\",\n",
    "    1: \"ISO Small\",\n",
    "    2: \"ISO Medium\", \n",
    "    3: \"ISO Large\",\n",
    "    4: \"clutter\",\n",
    "}\n",
    "n_class = len(class_dict.keys())\n",
    "\n",
    "clutter_dict = {\n",
    "#     0: \"30mm\",\n",
    "    0: \"20mm\",\n",
    "#     2: \"ISO Small Plate\",\n",
    "#     2: \"ISO Small Sphere\",\n",
    "#     4: \"Grenade\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.random.choice(n_class, ntrain)\n",
    "labels_test = np.random.choice(n_class, ntest)\n",
    "labels_valid = np.random.choice(n_class, nvalid)\n",
    "\n",
    "def generate_parameters(labels):\n",
    "    n = len(labels)\n",
    "    \n",
    "    depths = np.zeros(n)\n",
    "    \n",
    "    x = generate_random_variables(n, x_range)\n",
    "    y = generate_random_variables(n, y_range)\n",
    "\n",
    "    yaw = generate_random_variables(n, yaw_range)\n",
    "    pitch = generate_random_variables(n, pitch_range)\n",
    "    roll = generate_random_variables(n, roll_range)\n",
    "    \n",
    "    noise_amplitude = generate_random_variables(n, noise_amplitudes)\n",
    "    \n",
    "    polarizations = np.zeros(n, dtype=int)\n",
    "    clutter = np.zeros(n, dtype=int)\n",
    "    \n",
    "    for i in np.unique(labels):\n",
    "        ii = labels == i  # indices of the labels\n",
    "        \n",
    "        if class_dict[i] != \"not TOI\":\n",
    "            depths[ii] = generate_random_variables(ii.sum(), depth_ranges[i-1])\n",
    "            \n",
    "            if class_dict[i] == \"clutter\":\n",
    "                j = np.random.choice(len(clutter_dict.keys()), ii.sum())\n",
    "                clutter[ii] = j\n",
    "                for key, val in clutter_dict.items():\n",
    "                    jj = j == key\n",
    "                    if val == \"30mm\" or val == \"20mm\":\n",
    "                        polarizations[ii][jj] = np.ones(jj.sum())\n",
    "                    else: \n",
    "                        polarizations[ii][jj] = np.random.choice(len(ordnance[\"ISO Small\"][\"L3\"]), jj.sum())\n",
    "            else:\n",
    "                polarizations[ii] = np.random.choice(len(ordnance[class_dict[i]][\"L3\"]), ii.sum())\n",
    "                                 \n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"label\": labels,\n",
    "        \"depth\": depths,\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"z\": -depths,\n",
    "        \"yaw\": yaw,\n",
    "        \"pitch\": pitch,\n",
    "        \"roll\": roll, \n",
    "        \"noise_amplitude\": noise_amplitude,\n",
    "        \"polarizations\": polarizations,\n",
    "        \"clutter_type\": clutter\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train = generate_parameters(labels_train) \n",
    "params_test = generate_parameters(labels_test) \n",
    "params_valid = generate_parameters(labels_valid) \n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(18, 10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "i = 0\n",
    "for key, val in params_train.items():\n",
    "    for j in range(n_class):\n",
    "        inds = labels_train == j\n",
    "        if key != \"label\" and j == 0: \n",
    "            pass  # these parameters are irrelevant if there is no object\n",
    "        elif (key == \"clutter_type\") and (j == n_class-1):\n",
    "            ax[i].hist(val[inds], 10, color=f\"C{j}\", alpha=0.6)\n",
    "        elif key != \"clutter_type\":\n",
    "            ax[i].hist(val[inds], 10, color=f\"C{j}\", alpha=0.6)\n",
    "    ax[i].set_title(key)\n",
    "    i += 1\n",
    "\n",
    "ax[0].legend(list(class_dict.values()))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up forward simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_model(times, amplitude=0.1, slope=-1, intercept=-0.5):\n",
    "    return amplitude * np.exp(slope * np.log(times) + intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figuring out where all of the vertices are in coordinate system -- gives us location of vertices \n",
    "Tx_indices_rot, Rx_indices_rot = preCalcLoopCorners(\n",
    "    sensorinfo=sensorinfo, mnum=mnum, rlist=pos, pitch=pitch, roll=roll, yaw=yaw\n",
    ") \n",
    "\n",
    "# convienence object for inputs to fwd modelling \n",
    "st = FModParam(sensorinfo, pos, mnum, times, Tx_indices_rot, Rx_indices_rot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(parameters, st=st, mnum=mnum, ncycles=ncycles):\n",
    "    data = []\n",
    "    noise_data = []\n",
    "    for i, l in enumerate(parameters[\"label\"]):\n",
    "        if l == 0:\n",
    "            # this can later be replaced with random noise or other structured but uninteresting signal\n",
    "            V = np.zeros((mnum.max()+1, ncycles, len(times)))\n",
    "#             for j in range(mnum.max()+1):\n",
    "#                 mnum_noise = np.random.choice(mnum.max(), 1)[0]\n",
    "#                 d = noise_test_plot_mn[mnum_noise]\n",
    "#                 i_start = np.random.choice(d.shape[1], 1)[0]\n",
    "#                 if i_start > d.shape[1] - ncycles:\n",
    "#                     V[j, :, :] = np.hstack([\n",
    "#                         d[:, i_start:], \n",
    "#                         np.fliplr(d[:, -(ncycles - (d.shape[1]-i_start)):])\n",
    "#                     ]).T\n",
    "#                 else: \n",
    "#                     V[j, :, :] = d[:, i_start:i_start+ncycles].T\n",
    "        else: \n",
    "            xyz = np.r_[parameters[\"x\"][i], parameters[\"y\"][i], parameters[\"z\"][i]]\n",
    "            ypr = np.r_[parameters[\"yaw\"][i], parameters[\"pitch\"][i], parameters[\"roll\"][i]]\n",
    "            pi = parameters[\"polarizations\"][i]\n",
    "            \n",
    "            if class_dict[l] != \"clutter\":\n",
    "                L3 = ordnance[class_dict[l]][\"L3\"][pi]\n",
    "                L2 = ordnance[class_dict[l]][\"L2\"][pi]\n",
    "                L1 = ordnance[class_dict[l]][\"L1\"][pi]\n",
    "            else:\n",
    "                clutter_type = parameters[\"clutter_type\"][i]\n",
    "                clutter_name = clutter_dict[clutter_type]\n",
    "                if clutter_name in [\"30mm\", \"20mm\"]:\n",
    "                    L3 = ordnance[clutter_name][\"L3\"][pi]\n",
    "                    L2 = ordnance[clutter_name][\"L2\"][pi]\n",
    "                    L1 = ordnance[clutter_name][\"L1\"][pi]\n",
    "                elif clutter_name == \"ISO Small Plate\":\n",
    "                    L3 = ordnance[\"ISO Small\"][\"L3\"][pi]\n",
    "                    L2 = ordnance[\"ISO Small\"][\"L3\"][pi] # L2 == L3 is like a plate\n",
    "                    L1 = ordnance[\"ISO Small\"][\"L1\"][pi]\n",
    "                elif clutter_name == \"ISO Small Sphere\":\n",
    "                    L3 = ordnance[\"ISO Small\"][\"L1\"][pi]\n",
    "                    L2 = ordnance[\"ISO Small\"][\"L1\"][pi] # L1 == L2 == L3 is a sphere\n",
    "                    L1 = ordnance[\"ISO Small\"][\"L1\"][pi] # L1 == L2 == L3 is a sphere\n",
    "                                  \n",
    "\n",
    "            mod = Model(xyz=xyz, gba=ypr, l3=L3, l2=L2, l1=L1, times=times)\n",
    "            V = forwardWithQ(mod, st) # nT/s (some version of db/dt)\n",
    "            V = V.reshape(ncycles, mnum.max()+1, len(times))\n",
    "            V = np.swapaxes(V, 0, 1)\n",
    "        data.append(V)\n",
    "        noise_data.append(\n",
    "            V + \n",
    "            np.random.randn(np.prod(V.shape)).reshape(V.shape) * \n",
    "            noise_model(times, amplitude=parameters[\"noise_amplitude\"][i])\n",
    "        )\n",
    "    return data, noise_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_clean_train, data_train = run_simulations(params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_clean_test, data_test = run_simulations(params_test)\n",
    "data_clean_valid, data_valid = run_simulations(params_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_scaling = (times)\n",
    "time_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_train = [d*time_scaling for d in data_train]\n",
    "scaled_data_test = [d*time_scaling for d in data_test]\n",
    "scaled_data_valid = [d*time_scaling for d in data_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# for a, data, labels, title in zip(\n",
    "#     ax, \n",
    "#     [data_train, data_test, data_valid], \n",
    "#     [labels_train, labels_test, labels_valid],\n",
    "#     [\"train\", \"test\", \"valid\"]\n",
    "# ):\n",
    "#     data = np.stack(data, 0)\n",
    "#     for j in range(n_class):\n",
    "#         inds = labels == j\n",
    "#         a.hist(np.log10(np.abs(data[inds, :, :, :]) + 1e-12).flatten(), 100, color=f\"C{j}\", alpha=0.6)\n",
    "#         a.set_title(title)\n",
    "#         a.set_ylim([0, data.shape[0]*2e3])\n",
    "\n",
    "# ax[2].legend(list(class_dict.values()))\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# for a, data, labels, title in zip(\n",
    "#     ax, \n",
    "#     [scaled_data_train, scaled_data_test, scaled_data_valid], \n",
    "#     [labels_train, labels_test, labels_valid],\n",
    "#     [\"train\", \"test\", \"valid\"]\n",
    "# ):\n",
    "#     data = np.stack(data, 0)\n",
    "#     for j in range(n_class):\n",
    "#         inds = labels == j\n",
    "#         a.hist(np.log10(np.abs(data[inds, :, :, :])+1e-12).flatten(), 100, color=f\"C{j}\", alpha=0.6)\n",
    "#         a.set_title(title)\n",
    "#         a.set_ylim([0, data.shape[0]*2e3])\n",
    "\n",
    "# ax[2].legend(list(class_dict.values()))\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(d, eps=1e-12): \n",
    "    dd = d.reshape(d.shape[0], np.prod(d.shape[1:]), order=\"F\")\n",
    "    normalize_by = np.max(np.abs(dd), 1) + eps\n",
    "    return (dd.T/normalize_by).T.reshape(d.shape, order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_train = normalize_data(np.stack(scaled_data_train, 0))\n",
    "normalized_data_test = normalize_data(np.stack(scaled_data_test, 0))\n",
    "normalized_data_valid = normalize_data(np.stack(scaled_data_valid, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# for a, data, labels, title in zip(\n",
    "#     ax, \n",
    "#     [normalized_data_train, normalized_data_test, normalized_data_valid], \n",
    "#     [labels_train, labels_test, labels_valid],\n",
    "#     [\"train\", \"test\", \"valid\"]\n",
    "# ):\n",
    "# #     data = np.stack(data, 0)\n",
    "#     for j in range(n_class):\n",
    "#         inds = labels == j\n",
    "#         a.hist(np.log10(np.abs(data[inds, :, :, :])+1e-12).flatten(), 100, color=f\"C{j}\", alpha=0.6)\n",
    "#         a.set_title(title)\n",
    "#         a.set_ylim([0, data.shape[0]*2e3])\n",
    "\n",
    "# ax[2].legend(list(class_dict.values()))\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.swapaxes(pos.reshape(ncycles, mnum.max()+1, 3), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dind = 17\n",
    "d = data_train[dind]\n",
    "\n",
    "print(f\"class: {class_dict[labels_train[dind]]}, data shape: {d.shape}\")\n",
    "\n",
    "params_train.iloc[dind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx_num, rx_num, rx_comp, rxcube\n",
    "sensor_table = sensorinfo.measNum2TxRxRxcCube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sensor_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nrx = 11\n",
    "# fig, ax = plt.subplots(nrx, 3, figsize=(11, 1.5*nrx))\n",
    "# fig.subplots_adjust(wspace=0.3, hspace=0.15)\n",
    "\n",
    "# plot_tx = 1\n",
    "# tx_inds = (sensor_table[:, 0] == plot_tx)\n",
    "\n",
    "# cm = plt.get_cmap('magma')\n",
    "# c_norm = Normalize(vmin=times.min(), vmax=times.max())\n",
    "# scalar_map = cmap.ScalarMappable(norm=c_norm, cmap=cm)\n",
    "# scalar_map.set_array([])\n",
    "\n",
    "# d = normalized_data_train[dind, :, :, :]\n",
    "\n",
    "# for rx in range(nrx):\n",
    "#     ii = tx_inds & (sensor_table[:, 3] == rx)\n",
    "#     for a, comp in zip(ax[rx, :], [0, 1, 2]):\n",
    "#         mnumind = np.where( ii & (sensor_table[:, 2] == comp))[0][0]\n",
    "#         for t in range(len(times)):\n",
    "#             a.plot(\n",
    "#                 pos[mnumind, :, 1], d[mnumind, :, t], \n",
    "#                 color=scalar_map.to_rgba(times[t]), marker=\".\", lw=0.5, alpha=0.4, ms=3\n",
    "#             )\n",
    "\n",
    "#         a.grid(which=\"both\", alpha=0.4)\n",
    "#         if rx < nrx-1:\n",
    "#             a.set_xticklabels('')\n",
    "#         else:\n",
    "#             a.set_xlabel(\"along-line (m)\")\n",
    "        \n",
    "#         if rx == 0:\n",
    "#             if comp == 0:\n",
    "#                 a.set_title(\"x-component\")\n",
    "#             elif comp == 1:\n",
    "#                 a.set_title(\"y-component\")\n",
    "#             elif comp == 2:\n",
    "#                 a.set_title(\"z-component\")\n",
    "        \n",
    "#         if comp == 0:\n",
    "#             a.set_ylabel(f\"rx {rx}\")\n",
    "#             a.yaxis.set_label_coords(-0.3, 0.5)\n",
    "            \n",
    "# # plt.tight_layout()\n",
    "# cbar_ax = fig.add_axes([0.18, 0.07, 0.65, 0.005])\n",
    "# cb = plt.colorbar(scalar_map, cbar_ax, orientation=\"horizontal\")\n",
    "# cb.set_label('time (ms)')\n",
    "\n",
    "# fig.suptitle(f\"Transmitter {plot_tx+1}, Target {dind}: {class_dict[labels_train[dind]]}\", y=0.92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrx = 11\n",
    "# fig, ax = plt.subplots(nrx, 3, figsize=(11, 1.5*nrx))\n",
    "# fig.subplots_adjust(wspace=0.3, hspace=0.15)\n",
    "\n",
    "# plot_tx = 0\n",
    "# tx_inds = (sensor_table[:, 0] == plot_tx)\n",
    "\n",
    "# d = scaled_data_train[dind]\n",
    "\n",
    "# for rx in range(nrx):\n",
    "#     ii = tx_inds & (sensor_table[:, 3] == rx)\n",
    "#     plotme = d[mnumind, :, :].T\n",
    "#     for a, comp in zip(ax[rx, :], [0, 1, 2]):\n",
    "#         mnumind = np.where( ii & (sensor_table[:, 2] == comp))[0][0]\n",
    "#         out = a.pcolormesh(\n",
    "#             pos[mnumind, :, 1], times, \n",
    "#             plotme, cmap=\"RdBu\", vmin=-(np.abs(plotme.max())).max(), vmax=(np.abs(plotme.max())).max()\n",
    "#         )\n",
    "\n",
    "# #         a.grid(which=\"both\", alpha=0.4)\n",
    "# #         a.set_aspect(1)\n",
    "#         if rx < nrx-1:\n",
    "#             a.set_xticklabels('')\n",
    "#         else:\n",
    "#             a.set_xlabel(\"along-line (m)\")\n",
    "        \n",
    "#         if rx == 0:\n",
    "#             if comp == 0:\n",
    "#                 a.set_title(\"x-component\")\n",
    "#             elif comp == 1:\n",
    "#                 a.set_title(\"y-component\")\n",
    "#             elif comp == 2:\n",
    "#                 a.set_title(\"z-component\")\n",
    "        \n",
    "#         if comp == 0:\n",
    "#             a.set_ylabel(f\"rx {rx}\")\n",
    "#             a.yaxis.set_label_coords(-0.3, 0.5)\n",
    "\n",
    "# cbar_ax = fig.add_axes([0.18, 0.07, 0.65, 0.005])\n",
    "# cb = plt.colorbar(out, cbar_ax, orientation=\"horizontal\")\n",
    "# cb.set_label('signal')\n",
    "\n",
    "# fig.suptitle(f\"Transmitter {plot_tx+1}, Target {dind}: {class_dict[labels_train[dind]]}\", y=0.92)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a ConvNet classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_scaled = False\n",
    "use_normalized = True\n",
    "\n",
    "# normalize_unit = True\n",
    "\n",
    "if use_scaled is True: \n",
    "    X_train = torch.from_numpy(np.float32(np.stack(scaled_data_train, axis=0)))\n",
    "    X_test = torch.from_numpy(np.float32(np.stack(scaled_data_test, axis=0)))\n",
    "    X_valid = torch.from_numpy(np.float32(np.stack(scaled_data_valid, axis=0)))\n",
    "\n",
    "elif use_normalized is True: \n",
    "    X_train = torch.from_numpy(np.float32(normalized_data_train))\n",
    "    X_test = torch.from_numpy(np.float32(normalized_data_test))\n",
    "    X_valid = torch.from_numpy(np.float32(normalized_data_valid))\n",
    "\n",
    "else: \n",
    "    X_train = torch.from_numpy(np.float32(np.stack(data_train, axis=0)))\n",
    "    X_test = torch.from_numpy(np.float32(np.stack(data_test, axis=0)))\n",
    "    X_valid = torch.from_numpy(np.float32(np.stack(data_valid, axis=0)))\n",
    "\n",
    "C_train = torch.from_numpy(np.float32(labels_train)).long()\n",
    "C_test = torch.from_numpy(np.float32(labels_test)).long()\n",
    "C_valid = torch.from_numpy(np.float32(labels_valid)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, network_geometry):\n",
    "        super().__init__()\n",
    "        self.network_geometry=network_geometry\n",
    "        self.nt = len(network_geometry)\n",
    "        self.batch_norm = nn.BatchNorm2d(network_geometry[0])\n",
    "    \n",
    "    def forward(self, X, K, batch_norm, bias=None): \n",
    "        X = self.batch_norm(X)\n",
    "        if bias is None:\n",
    "            bias = [None]*len(K)\n",
    "        for i, Ki, bn, b in zip(range(self.nt), K, batch_norm, bias):\n",
    "            z = functional.conv2d(X, Ki, stride=1, padding=1, bias=b)\n",
    "            z = bn(z)\n",
    "            z = functional.relu(z)\n",
    "            z = functional.max_pool2d(z, 3, stride=1, padding=1)\n",
    "            X = z\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 165\n",
    "# n_class = 4\n",
    "layer_geometries = [in_channels, 33, 11] #11, 11, 11]\n",
    "nlayers = len(layer_geometries)\n",
    "initial_scaling = 1e-3\n",
    "\n",
    "width = nloc//ntx\n",
    "ntimes = len(times)\n",
    "\n",
    "# initialize K's and batch normalizations\n",
    "K = []\n",
    "batch_norm = []\n",
    "bias = None #[]\n",
    "for i in range(nlayers-1):\n",
    "    n_in = layer_geometries[i]\n",
    "    n_out = layer_geometries[i+1]\n",
    "    Ki = nn.Parameter(torch.Tensor(n_out, n_in, 3, 3))\n",
    "    Ki.data = torch.randn(n_out, n_in, 3, 3)  * initial_scaling\n",
    "    K.append(Ki)\n",
    "    \n",
    "#     bi = nn.Parameter(torch.Tensor(n_out))\n",
    "#     bi.data = torch.randn(n_out) * initial_scaling\n",
    "#     bias.append(bi)\n",
    "    \n",
    "    batch_norm.append(nn.BatchNorm2d(n_out))\n",
    "\n",
    "# initialize classifier W\n",
    "coarsening = (len(layer_geometries)  - 1)*0\n",
    "W = nn.Parameter(torch.Tensor(layer_geometries[-1]*(width-coarsening)*(ntimes-coarsening), n_class))\n",
    "W.data = torch.randn(layer_geometries[-1]*(width-coarsening)*(ntimes-coarsening), n_class)*initial_scaling\n",
    "\n",
    "b = nn.Parameter(torch.randn(n_class)*initial_scaling, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(layer_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the network\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = net(X_train, K, batch_norm, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "def classify(X, W, b): \n",
    "    n = W.shape\n",
    "    X = X.view(-1, n[0])\n",
    "    S = torch.matmul(X, W) + b.unsqueeze(0)\n",
    "    S = functional.softmax(S, dim=1)\n",
    "    _, C = torch.max(S, axis=1)\n",
    "    return C, S\n",
    "\n",
    "def misfit(X, W, b, C):\n",
    "    n = W.shape\n",
    "    X = X.view(-1, n[0])\n",
    "    S = torch.matmul(X, W) + b.unsqueeze(0)\n",
    "\n",
    "    probs = functional.softmax(S, dim=1)\n",
    "    return loss_func(S, C), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parout = (\n",
    "    np.sum(np.array([np.prod(Ki.shape) for Ki in K])) + np.prod(W.shape) #  + np.sum(np.array([np.prod(bi.shape) for bi in bias]))\n",
    ")\n",
    "\n",
    "print('Total number of parameters', n_parout)\n",
    "print('Total number of data', labels_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(S, labels):\n",
    "    _, predicted = torch.max(S, dim=1)\n",
    "    total = np.prod(labels.size())\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, probs = misfit(out, W, b, C_train)\n",
    "print('Initial loss = ', loss.detach().numpy())    \n",
    "print(f'Check:log({n_class}) = ', np.log(n_class))\n",
    "\n",
    "print(f\"\\nInitial accuracy: {accuracy(probs, C_train)}\")\n",
    "print(f\"Check random    : {1/n_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(\n",
    "    [{'params': W}, {'params':b}, {'params': K}], #, {'params': bias}], \n",
    "    lr = 0.25, momentum=0\n",
    ")\n",
    "# lr_cooling = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss_train = []\n",
    "running_loss_test = []\n",
    "running_accuracy_train = []\n",
    "running_accuracy_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    g = 0.0\n",
    "    loss = 0.0\n",
    "    ind = 0\n",
    "    \n",
    "    while ind < X_train.shape[0]:    \n",
    "        optimizer.zero_grad()\n",
    "        # get the inputs\n",
    "        inputs = X_train[ind:ind+batch_size, :, :, :]\n",
    "        labels = C_train[ind:ind+batch_size]\n",
    "\n",
    "        # forward \n",
    "        x = net(inputs, K, batch_norm, bias)\n",
    "        lossi, _ = misfit(x, W, b, labels)\n",
    "        if i==0:\n",
    "            loss = lossi\n",
    "        else:\n",
    "            loss += lossi\n",
    "        lossi.backward()\n",
    "        optimizer.step()\n",
    "        ind += batch_size\n",
    "    \n",
    "#     if np.mod(epoch, 4) == 0 and epoch > 0: \n",
    "#         print(\"cooling lr\")\n",
    "#         for opt_param in optim.param_groups:\n",
    "#             opt_paral[\"lr\"] *= lr_cooling\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        xtest = net(X_test, K, batch_norm, bias)\n",
    "        loss_test, probs_test = misfit(xtest, W, b, C_test)\n",
    "\n",
    "        xtrain = net(X_train, K, batch_norm, bias)\n",
    "        loss_train, probs_train = misfit(xtrain, W, b, C_train)\n",
    "\n",
    "        accuracy_train = accuracy(probs_train, C_train)\n",
    "        accuracy_test = accuracy(probs_test, C_test)\n",
    "\n",
    "        g = torch.norm(W.grad) + sum([torch.norm(Ki.grad) for Ki in K]) + torch.norm(b.grad)\n",
    "\n",
    "        running_loss_train.append(loss_train)\n",
    "        running_loss_test.append(loss_test)\n",
    "        running_accuracy_train.append(accuracy_train)\n",
    "        running_accuracy_test.append(accuracy_test)\n",
    "     \n",
    "        print(f'{epoch:3d}  {loss:2.3e}  {loss_test:2.3e} {g:2.3e} {accuracy_train:0.3f}  {accuracy_test:0.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "x_train_net = net(X_train, K, batch_norm, bias)\n",
    "_, probs_train = misfit(x_train_net, W, b, C_train)\n",
    "_, pred_train = torch.max(probs_train, dim=1)\n",
    "\n",
    "# test set\n",
    "x_test_net = net(X_test, K, batch_norm, bias)\n",
    "_, probs_test = misfit(x_test_net, W, b, C_test)\n",
    "_, pred_test = torch.max(probs_test, dim=1)\n",
    "\n",
    "# validation set\n",
    "x_valid_net = net(X_valid, K, batch_norm, bias)\n",
    "_, probs_valid = misfit(x_valid_net, W, b, C_valid)\n",
    "_, pred_valid = torch.max(probs_valid, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training accuracy: {accuracy(probs_train, C_train):1.4f}\")\n",
    "print(f\"test accuracy    : {accuracy(probs_test, C_test):1.4f}\")\n",
    "print(f\"valid accuracy   : {accuracy(probs_valid, C_valid):1.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where it struggled\n",
    "\n",
    "def get_mislabeled(S, labels):\n",
    "    _, predicted = torch.max(S, dim=1)\n",
    "    incorrect = (predicted != labels)\n",
    "    return incorrect.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_train = get_mislabeled(probs_train, C_train)\n",
    "mislabeled_test = get_mislabeled(probs_test, C_test)\n",
    "mislabeled_valid = get_mislabeled(probs_valid, C_valid)\n",
    "\n",
    "_, pred_labels_train = torch.max(probs_train, dim=1)\n",
    "_, pred_labels_test = torch.max(probs_test, dim=1)\n",
    "_, pred_labels_valid = torch.max(probs_valid, dim=1)\n",
    "\n",
    "print(f\"Mislabeled. train: {mislabeled_train.sum()}, test: {mislabeled_test.sum()}, valid: {mislabeled_valid.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(true_labels, predicted_labels):\n",
    "    n = len(np.unique(true_labels))\n",
    "    M = np.zeros((n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            M[i, j] = sum((true_labels == i) & (predicted_labels == j))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rows = True: {class_dict.values()}\")\n",
    "print(f\"cols = Pred: {class_dict.values()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(C_train, pred_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(C_test, pred_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(C_valid, pred_labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds, vals = torch.max(probs_valid[mislabeled_valid, :], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].semilogy(1/len(data_train) * np.array(running_loss_train), label=\"training\")\n",
    "ax[0].semilogy(1/len(data_test) * np.array(running_loss_test), label=\"test\")\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(running_accuracy_train, label=\"training\")\n",
    "ax[1].plot(running_accuracy_test, label=\"test\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for a in ax:\n",
    "    a.grid(which=\"both\")\n",
    "    a.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at features in network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 6\n",
    "\n",
    "print(probs_train[ind, :])\n",
    "print(f\"class: {class_dict[labels_train[ind]]}\")\n",
    "\n",
    "params_train.iloc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, len(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0 = []\n",
    "Z1 = []\n",
    "Z2 = []\n",
    "Z3 = []\n",
    "with torch.no_grad():\n",
    "    X = X_train[ind:ind+1, :, :, :]\n",
    "    for Ki, bn in zip(K, batch_norm):\n",
    "        print(Ki.shape)\n",
    "        z0 = functional.conv2d(X, Ki, stride=1, padding=1)\n",
    "        z1 = bn(z0)\n",
    "        z2 = functional.relu(z1)\n",
    "        z3 = functional.max_pool2d(z2, 3, stride=1, padding=1)\n",
    "        X = z2\n",
    "        Z0.append(z0)\n",
    "        Z1.append(z1)\n",
    "        Z2.append(z2)\n",
    "        Z3.append(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.colorbar(plt.pcolormesh(K[1][:, :, 2, 1].data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(33, 11, figsize = (11*0.5, 33*0.5))\n",
    "\n",
    "plotme = K[1].detach()\n",
    "vmax=np.abs(plotme.flatten()).max()\n",
    "vmin=-vmax\n",
    "print(vmin, vmax)\n",
    "\n",
    "for i in range(33):\n",
    "    for j in range(11):\n",
    "        a = ax[i, j]\n",
    "        a.set_xticklabels('')\n",
    "        a.set_yticklabels('')\n",
    "        a.pcolormesh(plotme[j, i, :, :], vmin=vmin, vmax=vmax, cmap=\"RdBu\")\n",
    "# K[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfeatures = 33\n",
    "# fig, ax = plt.subplots(11, 3, figsize=(11, 1.5*11))\n",
    "# fig.subplots_adjust(wspace=0.3, hspace=0.15)\n",
    "\n",
    "# cm = plt.get_cmap('magma')\n",
    "# c_norm = Normalize(vmin=times.min(), vmax=times.max())\n",
    "# scalar_map = cmap.ScalarMappable(norm=c_norm, cmap=cm)\n",
    "# scalar_map.set_array([])\n",
    "\n",
    "# plot_me = Z2[0][0, :, :, :]\n",
    "\n",
    "# for i, a in enumerate(ax.flatten()):\n",
    "#     for t in range(plot_me.shape[-1]):\n",
    "#         a.plot(\n",
    "#             plot_me[i, :, t], \n",
    "#             color=scalar_map.to_rgba(times[t]), marker=\".\", lw=0.5, alpha=0.4, ms=3\n",
    "#         )\n",
    "\n",
    "#         a.grid(which=\"both\", alpha=0.4)\n",
    "#         if rx < nrx-1:\n",
    "#             a.set_xticklabels('')\n",
    "            \n",
    "# # plt.tight_layout()\n",
    "# cbar_ax = fig.add_axes([0.18, 0.07, 0.65, 0.005])\n",
    "# cb = plt.colorbar(scalar_map, cbar_ax, orientation=\"horizontal\")\n",
    "# cb.set_label('time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfeatures = 11\n",
    "# fig, ax = plt.subplots(11, 1, figsize=(4, 1.5*11))\n",
    "# fig.subplots_adjust(wspace=0.3, hspace=0.15)\n",
    "\n",
    "# cm = plt.get_cmap('magma')\n",
    "# c_norm = Normalize(vmin=times.min(), vmax=times.max())\n",
    "# scalar_map = cmap.ScalarMappable(norm=c_norm, cmap=cm)\n",
    "# scalar_map.set_array([])\n",
    "\n",
    "# plot_me = Z2[1][0, :, :, :]\n",
    "\n",
    "# for i, a in enumerate(ax.flatten()):\n",
    "#     for t in range(plot_me.shape[-1]):\n",
    "#         a.plot(\n",
    "#             plot_me[i, :, t], \n",
    "#             color=scalar_map.to_rgba(times[t]), marker=\".\", lw=0.5, alpha=0.4, ms=3\n",
    "#         )\n",
    "\n",
    "#         a.grid(which=\"both\", alpha=0.4)\n",
    "#         if rx < nrx-1:\n",
    "#             a.set_xticklabels('')\n",
    "            \n",
    "# # plt.tight_layout()\n",
    "# cbar_ax = fig.add_axes([0.18, 0.07, 0.65, 0.005])\n",
    "# cb = plt.colorbar(scalar_map, cbar_ax, orientation=\"horizontal\")\n",
    "# cb.set_label('time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_plot = []\n",
    "with torch.no_grad():\n",
    "    for i in range(n_class):\n",
    "        w_plot.append(W[:, i].view(Z2[1].shape).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = 11\n",
    "fig, ax = plt.subplots(11, len(w_plot), figsize=(4*len(w_plot), 1.5*11))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.15)\n",
    "\n",
    "cm = plt.get_cmap('magma')\n",
    "c_norm = Normalize(vmin=times.min(), vmax=times.max())\n",
    "scalar_map = cmap.ScalarMappable(norm=c_norm, cmap=cm)\n",
    "scalar_map.set_array([])\n",
    "\n",
    "for ii, w in enumerate(w_plot):\n",
    "    for i, a in enumerate(ax[:, ii]):\n",
    "        for t in range(w.shape[-1]):\n",
    "            a.plot(\n",
    "                w[0, i, :, t], \n",
    "                color=scalar_map.to_rgba(times[t]), marker=\".\", lw=0.5, alpha=0.4, ms=3\n",
    "            )\n",
    "\n",
    "            a.grid(which=\"both\", alpha=0.4)\n",
    "            if ii < nfeatures-1:\n",
    "                a.set_xticklabels('')\n",
    "            \n",
    "# plt.tight_layout()\n",
    "cbar_ax = fig.add_axes([0.18, 0.07, 0.65, 0.005])\n",
    "cb = plt.colorbar(scalar_map, cbar_ax, orientation=\"horizontal\")\n",
    "cb.set_label('time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for a, probs, title in zip(ax, [probs_train, probs_test, probs_valid], [\"train\", \"test\", \"valid\"]):\n",
    "    a.set_title(title)\n",
    "    for i in range(n_class):\n",
    "        a.hist(probs[:, i].detach().numpy(), 100, color=f\"C{i}\", alpha=0.6)\n",
    "\n",
    "        a.set_ylim([0, 50])\n",
    "# ax[0].legend([class_dict[0], class_dict[1], class_dict[2]])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a line of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax_profile = 30\n",
    "nloc_profile = int(ymax_profile/dy)\n",
    "ncycles_profile = int(nloc_profile/ntx)\n",
    "\n",
    "y_profile = np.linspace(0, ymax_profile-dy, nloc_profile)\n",
    "x_profile = np.zeros(nloc_profile)\n",
    "z_profile = 0.28 * np.ones(nloc_profile)\n",
    "\n",
    "pitch_profile = np.zeros(nloc_profile)\n",
    "roll_profile = np.zeros(nloc_profile)\n",
    "yaw_profile = np.zeros(nloc_profile)  # moving north (sensor in typical orientation)\n",
    "\n",
    "txnum_profile = np.kron(np.ones(ncycles_profile), np.arange(ntx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sensor location coordinates to Rx locations\n",
    "pos_profile, mnum_profile = sensorCoords2RxCoords(\n",
    "    sensorinfo=sensorinfo,\n",
    "    x = x_profile, \n",
    "    y = y_profile, \n",
    "    z = z_profile, \n",
    "    pitch = pitch_profile, \n",
    "    roll = roll_profile, \n",
    "    yaw = yaw_profile,\n",
    "    txnum = txnum_profile\n",
    ")\n",
    "\n",
    "pitch_profile = np.concatenate([np.tile(x,pos_profile[i].shape[0]) for i,x in enumerate(pitch_profile)])\n",
    "roll_profile = np.concatenate([np.tile(x,pos_profile[i].shape[0]) for i,x in enumerate(roll_profile)])\n",
    "yaw_profile = np.concatenate([np.tile(x,pos_profile[i].shape[0]) for i,x in enumerate(yaw_profile)])\n",
    "pos_profile = np.concatenate(pos_profile,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_profile = np.r_[0.1, 0.25, 0.4, 0.1, 0.4, 0.7]\n",
    "params_profile = pd.DataFrame({\n",
    "    \"label\": [1, 1, 1, 2, 2, 2],\n",
    "    \"depth\": depths_profile,\n",
    "    \"x\": np.r_[0, 0, 0, 0, 0, 0],\n",
    "    \"y\": np.r_[2.5, 5, 10, 15, 20, 25],\n",
    "    \"z\": -depths_profile,\n",
    "    \"yaw\": 0*np.r_[0, np.pi/4, np.pi, 0, np.pi/4, np.pi/2],\n",
    "    \"pitch\": np.r_[0, 0, 0, 0, 0, 0],\n",
    "    \"roll\": np.r_[0, 0, 0, 0, 0, 0],\n",
    "    \"noise_amplitude\": 0.01*np.r_[1, 1, 1, 1, 1, 1],\n",
    "    \"polarizations\": [0, 0, 0, 0, 0, 0]\n",
    "})\n",
    "params_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figuring out where all of the vertices are in coordinate system -- gives us location of vertices \n",
    "Tx_indices_rot_profile, Rx_indices_rot_profile = preCalcLoopCorners(\n",
    "    sensorinfo=sensorinfo, mnum=mnum_profile, rlist=pos_profile, pitch=pitch_profile, roll=roll_profile, yaw=yaw_profile\n",
    ") \n",
    "\n",
    "# convienence object for inputs to fwd modelling \n",
    "st_profile = FModParam(sensorinfo, pos_profile, mnum_profile, times, Tx_indices_rot_profile, Rx_indices_rot_profile)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profile, noise_profile = run_simulations(params_profile, st=st_profile, mnum=mnum_profile, ncycles=ncycles_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profile = [d + n for d, n in zip(data_profile, noise_profile)]\n",
    "data_profile[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_profile = pos_profile[:, 0].reshape((165, ncycles_profile), order=\"F\")\n",
    "Y_profile = pos_profile[:, 1].reshape((165, ncycles_profile), order=\"F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_profile = sum(data_profile)\n",
    "D_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width_profile = int(ymax/y_spacing)\n",
    "w_step_profile = 5\n",
    "n_windows_profile = (X_profile.shape[1]-window_width_profile)//w_step_profile\n",
    "print(n_windows_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = D_profile.shape\n",
    "net_data_profile = np.zeros((n_windows_profile, n[0], window_width_profile, n[2]))\n",
    "net_y_profile = np.zeros((n_windows_profile, n[0], window_width_profile))\n",
    "net_x_profile = np.zeros((n_windows_profile, n[0], window_width_profile))\n",
    "\n",
    "for i in range(n_windows_profile):\n",
    "    inds = slice(i*w_step_profile,i*w_step_profile+window_width_profile)\n",
    "    net_x_profile[i, :, :] = X_profile[:, inds]\n",
    "    net_y_profile[i, :, :] = Y_profile[:, inds]\n",
    "    net_data_profile[i, :, :, :] = D_profile[:, inds, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_scaled is True or use_normalized is True:\n",
    "    print(\"scaling by time\")\n",
    "    net_data_profile_torch = net_data_profile*time_scaling\n",
    "else:\n",
    "    net_data_profile_torch = net_data_profile\n",
    "    \n",
    "if use_normalized is True:\n",
    "    print(\"normalizing\")\n",
    "    net_data_profile_torch = normalize_data(net_data_profile_torch)\n",
    "\n",
    "net_data_profile_torch = torch.from_numpy(np.float32(net_data_profile_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_field = net(net_data_profile_torch, K, batch_norm, bias)\n",
    "    labels_profile, probs_profile = classify(out_field, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnum_plot = 12\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ordnance_labels = [\n",
    "    Line2D(\n",
    "        [0], [0], marker='o', color=\"w\", \n",
    "        markerfacecolor=f'C{int(key)}', markeredgecolor=f'C{int(key)}', \n",
    "        label=value\n",
    "    ) for key, value in class_dict.items()\n",
    "]\n",
    "\n",
    "ordnance_labels = ordnance_labels + [\n",
    "    Line2D(\n",
    "        [0], [0], marker='s', color=\"w\", \n",
    "        markerfacecolor=f'C{int(key)}', markeredgecolor=f'C{int(key)}', \n",
    "        label=f\"true {class_dict[key]}\"\n",
    "    ) for key in [1, 2]\n",
    "]\n",
    "\n",
    "ax.plot(Y_profile[mnum_plot, :], D_profile[mnum_plot, :, :], '.', color=\"k\", alpha=0.3)\n",
    "ax.grid(\"both\", alpha=0.4)\n",
    "\n",
    "for i in range(net_data_profile.shape[0]):\n",
    "    y_plot = np.mean(net_y_profile[i, mnum_plot, :])\n",
    "    ax.plot(y_plot, 0, f'C{labels_profile[i]}o')\n",
    "\n",
    "txnumplot = sensor_table[mnum_plot, 0]\n",
    "rxnumplot = sensor_table[mnum_plot, 3]\n",
    "rxcompplot = sensor_table[mnum_plot, 2]\n",
    "ax.set_title(f\"Transmitter: {txnumplot}, Receiver: {rxnumplot}, {rxcompplot}-component\")\n",
    "ax.set_xlabel(\"along-line distance (m)\")\n",
    "\n",
    "for i in range(len(depths_profile)):\n",
    "    label = params_profile[\"label\"][i]\n",
    "    ax.plot(params_profile[\"y\"][i], -1, f\"sC{label}\")\n",
    "\n",
    "ax.legend(handles=ordnance_labels)\n",
    "\n",
    "# ax.set_xlim([12.5, 17.5])\n",
    "ax.set_ylim([-2, 8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_class):\n",
    "    plt.plot(probs_profile[:, i], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load up a line from the test site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../UXO_protected/data-blacktusk\"\n",
    "groundtruth = pd.read_excel(os.path.sep.join([data_directory, \"groundtruth_Testplot5F.xlsx\"]), nrows=93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5 functions\n",
    "def proc_attr(inp):\n",
    "    \"\"\"HDF5 - Process attributes for the input group and return them in a dictionary.\"\"\"\n",
    "    dic = {}\n",
    "    for att in inp.attrs.keys():\n",
    "        if inp.attrs[att].dtype.char == 'S':\n",
    "            dic[att] = [x.strip() for x in inp.attrs[att].tostring().decode('ascii').split(',')]\n",
    "        else:\n",
    "            dic[att] = inp.attrs[att][0] if isinstance(inp.attrs[att],np.ndarray) and inp.attrs[att].size==1 else inp.attrs[att]\n",
    "    return dic\n",
    "    pass\n",
    "\n",
    "def proc_group(inp):\n",
    "    \"\"\"HDF5 - A recursive function for reading datasets and attributes into a dictionary.\"\"\"\n",
    "    dic = {}\n",
    "    dic.update(proc_attr(inp))\n",
    "    for key in inp.keys():\n",
    "        if isinstance(inp[key], h5py.Group):\n",
    "            dic.update({key:proc_group(inp[key])})\n",
    "        else:\n",
    "            dic[key] = inp[key][()]\n",
    "        pass\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = os.path.join(data_directory, 'Testplot5F.h5')\n",
    "f = h5py.File(dfile, 'r')\n",
    "dic = proc_group(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sensorinfo structure\n",
    "sensor_name = dic['SensorName'][0]\n",
    "sensor_config = dic['SensorConfig'][0]\n",
    "afile = os.path.join(code_dir,'config','sensor_definitions','{}___{}.yaml'.format(sensor_name,sensor_config))\n",
    "sensorinfo = SensorInfo.fromYAML(afile)[0]\n",
    "\n",
    "print(sensor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_keys = [\n",
    "    't154', 't171', 't190', 't211', 't235', 't262', 't292', 't325', 't361', \n",
    "    't401', 't446', 't496', 't551', 't613', 't681', 't758', 't842', 't936', \n",
    "    't1040', 't1156', 't1286', 't1429', 't1588', 't1764', 't1961', 't2178', 't2420' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_dict = dic[\"XYZ\"]\n",
    "ch = xyz_dict['Info']['ChannelNames']\n",
    "datai = xyz_dict['Info']['Data']['ChannelIndex'].flatten().astype(int)-1 # These are indices to the decay data\n",
    "yawi = int(xyz_dict['Info']['Yaw']['ChannelIndex'])-1 # index to the yaw channel\n",
    "pitchi = int(xyz_dict['Info']['Pitch']['ChannelIndex'])-1 \n",
    "rolli = int(xyz_dict['Info']['Roll']['ChannelIndex'])-1 \n",
    "mni = int(xyz_dict[\"Info\"][\"MeasNum\"][\"ChannelIndex\"])-1\n",
    "easting = int(xyz_dict[\"Info\"][\"Easting\"][\"ChannelIndex\"])-1\n",
    "northing = int(xyz_dict[\"Info\"][\"Northing\"][\"ChannelIndex\"])-1\n",
    "linesi = int(xyz_dict[\"Info\"][\"Line\"][\"ChannelIndex\"])-1\n",
    "rx_num = int(xyz_dict[\"Info\"][\"RxNum\"][\"ChannelIndex\"])-1\n",
    "tx_num = int(xyz_dict[\"Info\"][\"TxNum\"][\"ChannelIndex\"])-1\n",
    "rx_comp = int(xyz_dict[\"Info\"][\"RxCNum\"][\"ChannelIndex\"])-1\n",
    "rx_num = int(xyz_dict[\"Info\"][\"RxNum\"][\"ChannelIndex\"]) - 1\n",
    "\n",
    "times = np.array(dic['SensorTimes'].flatten())\n",
    "print(times.min(), times.max(), len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_raw = False\n",
    "if use_raw:\n",
    "    datai = np.array([i for i in range(len(ch)) if ch[i] in raw_data_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_data = xyz_dict[\"Data\"]\n",
    "mn = xyz_data[mni, :].astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_id = 1004 #, 1013]\n",
    "line_inds = xyz_data[linesi, :] == line_id\n",
    "print(sum(line_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(xyz_data[easting, line_inds], xyz_data[northing, line_inds], '.', alpha=0.1, ms=0.2)\n",
    "ax.plot(groundtruth[\"Easting\"], groundtruth[\"Northing\"], 'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local coordinates for the data set \n",
    "x0, y0 = np.mean(xyz_data[easting, :]), np.mean(xyz_data[northing, :])\n",
    "local_easting = xyz_data[easting, :] - x0\n",
    "local_northing = xyz_data[northing, :] - y0\n",
    "\n",
    "slope, intercept, _, _, _ = stats.linregress(local_easting, local_northing)\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.arctan(slope)+np.pi/2\n",
    "local_x = np.cos(theta) * local_easting + np.sin(theta) * local_northing\n",
    "local_y = -np.sin(theta) * local_easting + np.cos(theta) * local_northing\n",
    "\n",
    "\n",
    "local_ground_truth_easting = np.array(groundtruth[\"Easting\"] - x0)\n",
    "local_ground_truth_northing = np.array(groundtruth[\"Northing\"] - y0)\n",
    "local_ground_truth_x = np.cos(theta) * local_ground_truth_easting + np.sin(theta) * local_ground_truth_northing\n",
    "local_ground_truth_y = -np.sin(theta) * local_ground_truth_easting + np.cos(theta) * local_ground_truth_northing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = local_x[line_inds]\n",
    "line_y = local_y[line_inds]\n",
    "\n",
    "mn_line = mn[line_inds]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(line_x, line_y, '.', ms=0.1)\n",
    "ax.set_xlim(np.r_[-20, 20])\n",
    "ax.set_title(\"rotated coordinate system\")\n",
    "\n",
    "ax.plot(local_ground_truth_x, local_ground_truth_y, 'k.', label=\"ordnance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_inds_covered = (\n",
    "    (local_ground_truth_x >= line_x.min()) & (local_ground_truth_x <= line_x.max()) &\n",
    "    (local_ground_truth_y >= line_y.min()) & (local_ground_truth_y <= line_y.max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_line = mn[line_inds]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 14))\n",
    "\n",
    "for plot_mn in range(11):\n",
    "\n",
    "    plot_inds = (mn_line==plot_mn)\n",
    "    ax.plot(line_x[plot_inds], line_y[plot_inds], '.', ms=1, label=plot_mn)\n",
    "\n",
    "ax.plot(\n",
    "    local_ground_truth_x[ord_inds_covered], local_ground_truth_y[ord_inds_covered], 'k.', label=\"ordnance\", ms=10\n",
    ")\n",
    "lgnd = ax.legend()\n",
    "[h._legmarker.set_markersize(6) for h in lgnd.legendHandles]\n",
    "\n",
    "# ax.set_xlim([-12, -2])\n",
    "ax.set_ylim([-10, 10])\n",
    "ax.grid(\"both\", alpha = 0.4)\n",
    "ax.set_aspect(0.5)\n",
    "ax.set_xlabel(\"x (m)\")\n",
    "ax.set_ylabel(\"y (m)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target data shape 165 X nlocs X nTimes \n",
    "# grab by measurement number \n",
    "# then re-stack\n",
    "\n",
    "x_locs = []\n",
    "y_locs = []\n",
    "data_list = []\n",
    "line_data = xyz_data[datai, :][:, line_inds]\n",
    "\n",
    "for mnind in range(mn.max() + 1):\n",
    "    mninds = mn_line == mnind\n",
    "    x_locs.append(line_x[mninds])\n",
    "    y_locs.append(line_y[mninds])\n",
    "    data_list.append(line_data[:, mninds].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_locs = np.vstack(x_locs)\n",
    "Y_locs = np.vstack(y_locs)\n",
    "S_data = np.stack(data_list, axis=0)\n",
    "print(S_data.shape, X_locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(groundtruth[\"ItemBuried\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a representative synthetic \n",
    "# grab 1/3 of the objects\n",
    "\n",
    "synthetic_keys = [\"20mm\", \"Small ISO\", \"Medium ISO\", \"Large ISO\"]\n",
    "# synthetic_starts_with = [\"Native\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_ground_truth = {}\n",
    "synthetic_easting = []\n",
    "synthetic_northing = []\n",
    "synthetic_x = []\n",
    "synthetic_y = []\n",
    "synthetic_item = []\n",
    "synthetic_depth = []\n",
    "\n",
    "for i, item in enumerate(groundtruth[\"ItemBuried\"]):\n",
    "    if item in synthetic_keys or item.startswith(\"Native\"):\n",
    "        synthetic_easting.append(groundtruth[\"Easting\"][i])\n",
    "        synthetic_northing.append(groundtruth[\"Northing\"][i])\n",
    "        synthetic_x.append(local_ground_truth_x[i])\n",
    "        synthetic_y.append(local_ground_truth_y[i])\n",
    "        \n",
    "        d = groundtruth[\"Depth (m)\"][i]\n",
    "        if np.isnan(d):\n",
    "            d = 0.3 \n",
    "        synthetic_depth.append(d)\n",
    "        \n",
    "        if item.startswith(\"Native\"):\n",
    "            synthetic_item.append(\"Small ISO\")\n",
    "        else: \n",
    "            synthetic_item.append(item)\n",
    "\n",
    "synthetic_ground_truth = {\n",
    "    \"easting\": np.array(synthetic_easting),\n",
    "    \"northing\": np.array(synthetic_northing),\n",
    "    \"x\": np.array(synthetic_x), \n",
    "    \"y\": np.array(synthetic_y), \n",
    "    \"depth\": np.array(synthetic_depth),\n",
    "    \"item\": np.array(synthetic_item)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_subset = {\n",
    "    key: val[::3] for key, val in synthetic_ground_truth.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(xyz_data[easting], xyz_data[northing], '.', alpha=0.1, ms=0.2)\n",
    "ax.plot(synthetic_subset[\"easting\"], synthetic_subset[\"northing\"], 'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "depth = []\n",
    "x = []\n",
    "y = []\n",
    "yaw = []\n",
    "pitch = []\n",
    "roll = []\n",
    "polarizations = []\n",
    "\n",
    "\n",
    "for i, item in enumerate(synthetic_subset[\"item\"]):\n",
    "    if item in [\"Small ISO\", \"Native\"]:\n",
    "        label.append(1)\n",
    "    elif item == \"Medium ISO\":\n",
    "        label.append(2)\n",
    "    elif item == \"Large ISO\":\n",
    "        label.append(3)\n",
    "    elif item == \"20mm\":\n",
    "        label.append(4)\n",
    "    else: \n",
    "        print(item)\n",
    "    polarizations.append(0)\n",
    "    yaw.append(0)\n",
    "    pitch.append(0)\n",
    "    roll.append(0)\n",
    "\n",
    "params_synthetic = pd.DataFrame({\n",
    "    \"label\": np.array(label),\n",
    "    \"depth\": synthetic_subset[\"depth\"],\n",
    "    \"x\": synthetic_subset[\"x\"],\n",
    "    \"y\": synthetic_subset[\"y\"],\n",
    "    \"z\": -synthetic_subset[\"depth\"],\n",
    "    \"yaw\": np.zeros(len(label)),\n",
    "    \"pitch\": np.zeros(len(label)),\n",
    "    \"roll\": np.zeros(len(label)),\n",
    "    \"noise_amplitude\": 0.05*np.ones(len(label)),\n",
    "    \"polarizations\": polarizations,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnum_synthetic = xyz_data[mni, :].astype(int) - 1\n",
    "pos_synthetic = np.hstack([local_x[:, None], local_y[:, None], 0.28*np.ones((len(mnum_synthetic),1))])\n",
    "\n",
    "Tx_indices_rot, Rx_indices_rot = preCalcLoopCorners(\n",
    "    sensorinfo=sensorinfo, mnum=mnum_synthetic, rlist=pos_synthetic, \n",
    "    pitch=xyz_data[pitchi, :], roll=xyz_data[rolli, :], yaw=xyz_data[yawi, :]\n",
    ") \n",
    "\n",
    "# convienence object for inputs to fwd modelling \n",
    "st_synthetic = FModParam(sensorinfo, pos_synthetic, mnum_synthetic, times, Tx_indices_rot, Rx_indices_rot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(parameters, st, mnum):\n",
    "    data = None\n",
    "    noise_data = None\n",
    "    for i, l in enumerate(parameters[\"label\"]):\n",
    "        print(f\"{i}: simulating {l}\")\n",
    "        xyz = np.r_[parameters[\"x\"][i], parameters[\"y\"][i], parameters[\"z\"][i]]\n",
    "        ypr = np.r_[parameters[\"yaw\"][i], parameters[\"pitch\"][i], parameters[\"roll\"][i]]\n",
    "        pi = parameters[\"polarizations\"][i]\n",
    "\n",
    "        if class_dict[l] != \"clutter\":\n",
    "            L3 = ordnance[class_dict[l]][\"L3\"][pi]\n",
    "            L2 = ordnance[class_dict[l]][\"L2\"][pi]\n",
    "            L1 = ordnance[class_dict[l]][\"L1\"][pi]\n",
    "        else:\n",
    "            clutter_name = \"20mm\"\n",
    "            L3 = ordnance[clutter_name][\"L3\"][pi]\n",
    "            L2 = ordnance[clutter_name][\"L2\"][pi]\n",
    "            L1 = ordnance[clutter_name][\"L1\"][pi]\n",
    "\n",
    "        mod = Model(xyz=xyz, gba=ypr, l3=L3, l2=L2, l1=L1, times=times)\n",
    "        V = forwardWithQ(mod, st) # mV\n",
    "        if np.isnan(V).any():\n",
    "            print(i)\n",
    "            raise Exception \n",
    "        \n",
    "    #     n = V.shape()\n",
    "    #     V = V.reshape(ncycles, mnum.max()+1, len(times))\n",
    "    #     V = np.swapaxes(V, 0, 1)\n",
    "        noisy_V = (\n",
    "            V + \n",
    "            np.random.randn(np.prod(V.shape)).reshape(V.shape) * \n",
    "            noise_model(times, amplitude=parameters[\"noise_amplitude\"][i])\n",
    "        )\n",
    "        if data is None:\n",
    "            data = V\n",
    "            noise_data = noisy_V\n",
    "        else:\n",
    "            data = data + V\n",
    "            noise_data = noise_data + noisy_V\n",
    "    return data, noise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_synthetic.loc[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data, synthetic_data_noisy = generate_synthetic_data(\n",
    "    params_synthetic, st_synthetic, mnum_synthetic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_data[linesi,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = int(ymax/y_spacing)\n",
    "w_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(local_pos, mn, data):\n",
    "    classified_X = []\n",
    "    classified_Y = []\n",
    "    classified_probs = []\n",
    "    classified_classes = []\n",
    "\n",
    "    for line_id in np.unique(xyz_data[linesi, :]): \n",
    "        line_inds = xyz_data[linesi, :] == line_id\n",
    "\n",
    "        x_locs = []\n",
    "        y_locs = []\n",
    "        data_list = []\n",
    "        line_data = data[:, line_inds]\n",
    "        mn_line = mn[line_inds]\n",
    "        line_x = local_pos[line_inds, 0]\n",
    "        line_y = local_pos[line_inds, 1]\n",
    "\n",
    "        min_mn_num = np.inf\n",
    "        for mnind in range(mn.max() + 1):    \n",
    "            mninds = mn_line == mnind\n",
    "            if mninds.sum() < min_mn_num:\n",
    "                min_mn_num = mninds.sum()\n",
    "            x_locs.append(line_x[mninds])\n",
    "            y_locs.append(line_y[mninds])\n",
    "            data_list.append(line_data[:, mninds].T)\n",
    "\n",
    "        X_locs = np.vstack([x[:min_mn_num] for x in x_locs])\n",
    "        Y_locs = np.vstack([y[:min_mn_num] for y in y_locs])\n",
    "        S_data = np.stack([d[:min_mn_num, :] for d in data_list], axis=0)\n",
    "\n",
    "        n_windows = (X_locs.shape[1]-window_width)//w_step + 1\n",
    "        print(line_id, window_width, w_step, n_windows, X_locs.shape)\n",
    "\n",
    "        n = S_data.shape\n",
    "        net_data = np.zeros((n_windows, n[0], window_width, n[2]))\n",
    "        net_y = np.zeros((n_windows, n[0], window_width))\n",
    "        net_x = np.zeros((n_windows, n[0], window_width))\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            inds = slice(i*w_step,i*w_step+window_width)\n",
    "            net_x[i, :, :] = X_locs[:, inds]\n",
    "            net_y[i, :, :] = Y_locs[:, inds]\n",
    "            net_data[i, :, :, :] = S_data[:, inds, :]\n",
    "\n",
    "        if use_scaled or use_normalized:\n",
    "            net_data = net_data * time_scaling\n",
    "\n",
    "        if use_normalized:\n",
    "            net_data = normalize_data(net_data)\n",
    "\n",
    "        net_data_torch = torch.from_numpy(np.float32(net_data))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_field = net(net_data_torch, K, batch_norm, bias)\n",
    "            classes_field, probs_field = classify(out_field, W, b)\n",
    "\n",
    "        classified_X.append(net_x)\n",
    "        classified_Y.append(net_y)\n",
    "        classified_probs.append(probs_field.numpy())\n",
    "        classified_classes.append(classes_field.numpy())\n",
    "    return {\n",
    "        \"x\": classified_X,\n",
    "        \"y\": classified_Y,\n",
    "        \"probs\": np.vstack(classified_probs),\n",
    "        \"classes\": np.hstack(classified_classes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_clean = classify_data(pos_synthetic, mn, synthetic_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_X_means = np.hstack([cx.mean(1).mean(1) for cx in classify_clean[\"x\"]])\n",
    "synthetic_Y_means = np.hstack([cy.mean(1).mean(1) for cy in classify_clean[\"y\"]])\n",
    "\n",
    "synthetic_eastings  = np.cos(theta) * synthetic_X_means - np.sin(theta) * synthetic_Y_means + x0 \n",
    "synthetic_northings = np.sin(theta) * synthetic_X_means + np.cos(theta) * synthetic_Y_means + y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "plt.colorbar(ax.scatter(\n",
    "    synthetic_eastings, synthetic_northings, c=classify_clean[\"probs\"][:, 1] + classify_clean[\"probs\"][:, 2], marker='o', \n",
    "    cmap=\"viridis_r\", vmin=0., vmax=1, s=10, alpha=0.5\n",
    "    ), \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# for i, item in enumerate(synthetic_ground_truth[\"item\"]):\n",
    "#     if item == \"Small ISO\":\n",
    "#         ax.plot(synthetic_ground_truth[\"easting\"][i], synthetic_ground_truth[\"northing\"][i], 'ko', ms=5)\n",
    "#     elif item == \"Medium ISO\": \n",
    "#         ax.plot(synthetic_ground_truth[\"easting\"][i], synthetic_ground_truth[\"northing\"][i], 'ks', ms=5)\n",
    "#     elif item == \"Large ISO\":\n",
    "#         ax.plot(synthetic_ground_truth[\"easting\"][i], synthetic_ground_truth[\"northing\"][i], 'kv', ms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_width = int(ymax/y_spacing)\n",
    "# w_step = 5\n",
    "# n_windows = (X_locs.shape[1]-window_width)//w_step + 1\n",
    "# print(window_width, w_step, n_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = S_data.shape\n",
    "# net_data = np.zeros((n_windows, n[0], window_width, n[2]))\n",
    "# net_y = np.zeros((n_windows, n[0], window_width))\n",
    "# net_x = np.zeros((n_windows, n[0], window_width))\n",
    "\n",
    "# for i in range(n_windows):\n",
    "#     inds = slice(i*w_step,i*w_step+window_width)\n",
    "#     net_x[i, :, :] = X_locs[:, inds]\n",
    "#     net_y[i, :, :] = Y_locs[:, inds]\n",
    "#     net_data[i, :, :, :] = S_data[:, inds, :]\n",
    "\n",
    "# if use_scaled or use_normalized:\n",
    "#     net_data = net_data * time_scaling\n",
    "\n",
    "# if use_normalized:\n",
    "#     net_data = normalize_data(net_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_data_torch = torch.from_numpy(np.float32(net_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     out_field = net(net_data_torch, K, batch_norm, bias)\n",
    "#     labels_field, probs_field = classify(out_field, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnum_plot = 11\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "# ordnance_labels = [\n",
    "#     Line2D(\n",
    "#         [0], [0], marker='o', color=\"w\", \n",
    "#         markerfacecolor=f'C{int(key)}', markeredgecolor=f'C{int(key)}', \n",
    "#         label=value\n",
    "#     ) for key, value in class_dict.items()\n",
    "# ]\n",
    "\n",
    "# ordnance_labels.append(\n",
    "#     Line2D(\n",
    "#         [0], [0], marker='s', color=\"w\", \n",
    "#         markerfacecolor=f'C3', markeredgecolor=f'C3', \n",
    "#         label=\"ground truth\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# ax.plot(y_locs[mnum_plot], data_list[mnum_plot], '.', color=\"k\", alpha=0.3)\n",
    "# ax.grid(\"both\", alpha=0.4)\n",
    "\n",
    "# for i in range(net_data.shape[0]):\n",
    "#     y_plot = np.mean(net_y[i, mnum_plot, :])\n",
    "#     ax.plot(y_plot, 0, f'C{labels_field[i]}o')\n",
    "\n",
    "# txnumplot = sensor_table[mnum_plot, 0]\n",
    "# rxnumplot = sensor_table[mnum_plot, 3]\n",
    "# rxcompplot = sensor_table[mnum_plot, 2]\n",
    "# ax.set_title(f\"Line: {line_id}, Transmitter: {txnumplot}, Receiver: {rxnumplot}, {rxcompplot}-component\")\n",
    "# ax.set_xlabel(\"along-line distance (m)\")\n",
    "# ax.plot(local_ground_truth_y[ord_inds_covered], -5*(np.ones(ord_inds_covered.sum())) , \"sC6\")\n",
    "\n",
    "# ax.legend(handles=ordnance_labels)\n",
    "\n",
    "# ax.set_xlim([-20, 0])\n",
    "# ax.set_ylim([-10, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(probs_field, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth[\"ItemBuried\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth[:][ord_inds_covered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify the whole survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_lines = np.unique(xyz_data[linesi, :])\n",
    "# n_lines = len(unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_width = int(ymax/y_spacing)\n",
    "# w_step = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified_X = []\n",
    "# classified_Y = []\n",
    "# classified_probs = []\n",
    "# classified_classes = []\n",
    "\n",
    "# for line_id in unique_lines: \n",
    "#     line_inds = xyz_data[linesi, :] == line_id\n",
    "    \n",
    "#     x_locs = []\n",
    "#     y_locs = []\n",
    "#     data_list = []\n",
    "#     line_data = xyz_data[datai, :][:, line_inds]\n",
    "#     mn_line = mn[line_inds]\n",
    "#     line_x = local_x[line_inds]\n",
    "#     line_y = local_y[line_inds]\n",
    "\n",
    "#     min_mn_num = np.inf\n",
    "#     for mnind in range(mn.max() + 1):    \n",
    "#         mninds = mn_line == mnind\n",
    "#         if mninds.sum() < min_mn_num:\n",
    "#             min_mn_num = mninds.sum()\n",
    "#         x_locs.append(line_x[mninds])\n",
    "#         y_locs.append(line_y[mninds])\n",
    "#         data_list.append(line_data[:, mninds].T)\n",
    "    \n",
    "#     X_locs = np.vstack([x[:min_mn_num] for x in x_locs])\n",
    "#     Y_locs = np.vstack([y[:min_mn_num] for y in y_locs])\n",
    "#     S_data = np.stack([d[:min_mn_num, :] for d in data_list], axis=0)\n",
    "    \n",
    "#     n_windows = (X_locs.shape[1]-window_width)//w_step + 1\n",
    "#     print(line_id, window_width, w_step, n_windows, X_locs.shape)\n",
    "    \n",
    "#     n = S_data.shape\n",
    "#     net_data = np.zeros((n_windows, n[0], window_width, n[2]))\n",
    "#     net_y = np.zeros((n_windows, n[0], window_width))\n",
    "#     net_x = np.zeros((n_windows, n[0], window_width))\n",
    "\n",
    "#     for i in range(n_windows):\n",
    "#         inds = slice(i*w_step,i*w_step+window_width)\n",
    "#         net_x[i, :, :] = X_locs[:, inds]\n",
    "#         net_y[i, :, :] = Y_locs[:, inds]\n",
    "#         net_data[i, :, :, :] = S_data[:, inds, :]\n",
    "\n",
    "#     if use_scaled or use_normalized:\n",
    "#         net_data = net_data * time_scaling\n",
    "\n",
    "#     if use_normalized:\n",
    "#         net_data = normalize_data(net_data)\n",
    "        \n",
    "#     net_data_torch = torch.from_numpy(np.float32(net_data))\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         out_field = net(net_data_torch, K, batch_norm, bias)\n",
    "#         classes_field, probs_field = classify(out_field, W, b)\n",
    "    \n",
    "#     classified_X.append(net_x)\n",
    "#     classified_Y.append(net_y)\n",
    "#     classified_probs.append(probs_field)\n",
    "#     classified_classes.append(classes_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified_X_means = np.hstack([cx.mean(1).mean(1) for cx in classified_X])\n",
    "# classified_Y_means = np.hstack([cy.mean(1).mean(1) for cy in classified_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified_eastings  = np.cos(theta) * classified_X_means - np.sin(theta) * classified_Y_means + x0 \n",
    "# classified_northings = np.sin(theta) * classified_X_means + np.cos(theta) * classified_Y_means + y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified_probs_stack = np.vstack(classified_probs)\n",
    "# classified_classes_stack = np.hstack(classified_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# for key, val in class_dict.items():\n",
    "#     inds = classified_classes_stack == key\n",
    "#     ax.plot(\n",
    "#         classified_eastings[inds], classified_northings[inds], f\"C{key}o\", ms=3, label=val\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "# plt.colorbar(ax.scatter(\n",
    "#     classified_eastings, classified_northings, c=(classified_probs_stack[:, 1]), marker='o', \n",
    "#     cmap=\"Reds\", vmin=0.8, vmax=1, s=10, alpha=0.5\n",
    "#     ), \n",
    "#     ax=ax\n",
    "# )\n",
    "\n",
    "# for i, item in enumerate(groundtruth[\"ItemBuried\"]):\n",
    "#     if item == \"Small ISO\":\n",
    "#         ax.plot(groundtruth[\"Easting\"][i], groundtruth[\"Northing\"][i], 'ko', ms=5)\n",
    "#     elif item.startswith(\"Native\"): \n",
    "#         ax.plot(groundtruth[\"Easting\"][i], groundtruth[\"Northing\"][i], 'ks', ms=5)\n",
    "# #     elif item == \"Railway\":\n",
    "# #         ax.plot(groundtruth[\"Easting\"][i], groundtruth[\"Northing\"][i], 'kv', ms=4)\n",
    "# #     elif item == \"Large ISO\":\n",
    "# #         ax.plot(groundtruth[\"Easting\"][i], groundtruth[\"Northing\"][i], 'k^', ms=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(classified_probs_stack, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(groundtruth[\"ItemBuried\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth.loc[[i for i, g in enumerate(groundtruth[\"ItemBuried\"]) if g.startswith(\"Native\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
